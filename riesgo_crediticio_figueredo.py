# -*- coding: utf-8 -*-
"""Riesgo Crediticio - Figueredo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zzoj57D8ErNt32kzF4QrZmVspcBymy3E

# 1) Instalación de dependencias e Importación del DataSet
"""

import pandas as pd
import matplotlib.pyplot as plt
plt.style.use('default')
import numpy as np
import seaborn as sns
sns.set_palette("husl")
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import roc_auc_score
from sklearn.metrics import confusion_matrix, classification_report
import joblib
from google.colab import files


!pip install ucimlrepo
from ucimlrepo import fetch_ucirepo

#Código para importar el dataset (en la fuente):

# fetch dataset
statlog_german_credit_data = fetch_ucirepo(id=144)

# data (as pandas dataframes)
X = statlog_german_credit_data.data.features
y = statlog_german_credit_data.data.targets

# metadata
print(statlog_german_credit_data.metadata)

# variable information
print(statlog_german_credit_data.variables)
# fetch dataset
statlog_german_credit_data = fetch_ucirepo(id=144)

# data (as pandas dataframes)
X = statlog_german_credit_data.data.features
y = statlog_german_credit_data.data.targets

# metadata
print(statlog_german_credit_data.metadata)

# variable information
print(statlog_german_credit_data.variables)

"""# 2) Exploración y preprocesamiento de los datos

Primero podemos renombrar las columnas y variables de las features para poder entender mejor el tipo de datos que tenemos, ya que está todo con código y es dificil de interpretar a simple vista
"""

#Disclaimer: para armar este código, se utilizó la ayuda de Claude Sonnet 4 para poder replicar el código a todas las variables existentes de forma más rápida

#Renombro los encabezados de las variables

X = X.rename(columns={
    'Attribute1': 'account_status',
    'Attribute2': 'duration_in_months',
    'Attribute3': 'credit_history',
    'Attribute4': 'purpose',
    'Attribute5': 'credit_amount',
    'Attribute6': 'savings_account',
    'Attribute7': 'employment_duration',
    'Attribute8': 'installment_rate_pct',
    'Attribute9': 'personal_status_sex',
    'Attribute10': 'other_debtors',
    'Attribute11': 'residence_since',
    'Attribute12': 'property_type',
    'Attribute13': 'age_years',
    'Attribute14': 'other_installment_plans',
    'Attribute15': 'housing_type',
    'Attribute16': 'existing_credits_count',
    'Attribute17': 'job_type',
    'Attribute18': 'dependents_count',
    'Attribute19': 'telephone',
    'Attribute20': 'foreign_worker'
})

#Hago lo mismo pero ahora para las variables dentro

# Mapeo para account_status
account_status_map = {
    'A11': 'negative_balance',
    'A12': 'balance_0_to_200',
    'A13': 'balance_over_200',
    'A14': 'no_account'
}

# Mapeo para credit_history
credit_history_map = {
    'A30': 'no_credits_or_all_paid',
    'A31': 'all_paid_at_bank',
    'A32': 'existing_paid_duly',
    'A33': 'past_payment_delays',
    'A34': 'critical_account'
}

# Mapeo para purpose
purpose_map = {
    'A40': 'new_car',
    'A41': 'used_car',
    'A42': 'furniture_equipment',
    'A43': 'radio_tv',
    'A44': 'domestic_appliances',
    'A45': 'repairs',
    'A46': 'education',
    'A47': 'vacation',
    'A48': 'retraining',
    'A49': 'business',
    'A410': 'other_purpose'
}

# Mapeo para savings_account
savings_account_map = {
    'A61': 'savings_below_100',
    'A62': 'savings_100_to_500',
    'A63': 'savings_500_to_1000',
    'A64': 'savings_over_1000',
    'A65': 'no_savings'
}

# Mapeo para employment_duration
employment_duration_map = {
    'A71': 'unemployed',
    'A72': 'employed_less_1yr',
    'A73': 'employed_1_to_4yrs',
    'A74': 'employed_4_to_7yrs',
    'A75': 'employed_over_7yrs'
}

# Mapeo para personal_status_sex
personal_status_sex_map = {
    'A91': 'male_divorced',
    'A92': 'female_not_single',
    'A93': 'male_single',
    'A94': 'male_married_widowed',
    'A95': 'female_single'
}

# Mapeo para other_debtors
other_debtors_map = {
    'A101': 'none',
    'A102': 'co_applicant',
    'A103': 'guarantor'
}

# Mapeo para property_type
property_type_map = {
    'A121': 'real_estate',
    'A122': 'savings_insurance',
    'A123': 'car_other',
    'A124': 'no_property'
}

# Mapeo para other_installment_plans
other_installment_plans_map = {
    'A141': 'bank',
    'A142': 'stores',
    'A143': 'none'
}

# Mapeo para housing_type
housing_type_map = {
    'A151': 'rent',
    'A152': 'own',
    'A153': 'free'
}

# Mapeo para job_type
job_type_map = {
    'A171': 'unemployed_unskilled_nonresident',
    'A172': 'unskilled_resident',
    'A173': 'skilled_employee',
    'A174': 'management_selfemployed'
}

# Mapeo para telephone
telephone_map = {
    'A191': 'none',
    'A192': 'yes'
}

# Mapeo para foreign_worker
foreign_worker_map = {
    'A201': 'yes',
    'A202': 'no'
}


# Aplicación de los mapeos a las columnas correspondientes
X['account_status'] = X['account_status'].map(account_status_map)
X['credit_history'] = X['credit_history'].map(credit_history_map)
X['purpose'] = X['purpose'].map(purpose_map)
X['savings_account'] = X['savings_account'].map(savings_account_map)
X['employment_duration'] = X['employment_duration'].map(employment_duration_map)
X['personal_status_sex'] = X['personal_status_sex'].map(personal_status_sex_map)
X['other_debtors'] = X['other_debtors'].map(other_debtors_map)
X['property_type'] = X['property_type'].map(property_type_map)
X['other_installment_plans'] = X['other_installment_plans'].map(other_installment_plans_map)
X['housing_type'] = X['housing_type'].map(housing_type_map)
X['job_type'] = X['job_type'].map(job_type_map)
X['telephone'] = X['telephone'].map(telephone_map)
X['foreign_worker'] = X['foreign_worker'].map(foreign_worker_map)

"""Es conveniente combinar todo en un DataFrame para poder ver las características principales del mismo"""

df = pd.concat([y, X], axis=1)
df

print("=== INFORMACIÓN BÁSICA DEL DATASET ===")
print(f"Dimensiones: {df.shape}")
print(f"Total de registros: {df.shape[0]}")
print(f"Total de variables: {df.shape[1]}")
print("\n=== TIPO DE DATOS ===")
print(df.dtypes)

"""Podemos verificar que no haya valores nulos (aunque la documentación fuente del dataset diga que no hay), para asegurarnos de no tener problemas en el análisis"""

missing_data = df.isnull().sum()
print("=== VALORES FALTANTES ===")
print(missing_data)

"""Una buena idea sería hacer una exploración de la distribución de la variable objetivo para poder determinar si la muestra está balanceada."""

target_col = y.columns[0]
print(f"=== ANÁLISIS DE LA VARIABLE OBJETIVO: class ===")

# Distribución de clases
class_counts = df[target_col].value_counts()
class_props = df[target_col].value_counts(normalize=True)

print("Distribución de class:")
for class_val, count in class_counts.items():
    print(f"Clase {class_val}: {count} ({class_props[class_val]:.2%})")

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

# Gráfico de barras
class_counts.plot(kind='bar', ax=ax1, color=['paleturquoise', 'salmon'])
ax1.set_title('Distribución de Clases - Conteos')
ax1.set_xlabel('Clase')
ax1.set_ylabel('Cantidad')
ax1.tick_params(axis='x', rotation=0)

# Gráfico de torta
ax2.pie(class_counts.values, labels=class_counts.index, autopct='%1.1f%%',
        colors=['paleturquoise', 'salmon'])
ax2.set_title('Distribución de Clases - Proporciones')

plt.tight_layout()
plt.show()

"""Como podemos notar hay un desbalance muy grande en la muestra:

*   Un 70% son buenos pagadores
*   Un 30% son malos pagadores

Por este motivo, debemos tener en cuenta a la hora de revisar el modelo ya que tendremos la tendencia de que prediga buenos pagadores en su mayoría cuando pueden ser malos.

A su vez, tenemos que tener en cuenta que la documentación original del dataset tiene una matriz de costos explicando el costo de los errores en las predicciones.

Por lo que tenemos que tener cuidado con los clientes que el modelo catalogue como buenos pagadores, cuando en realidad son malos, ya que incurre en un costo mucho mayor ya que se otorga un crédito que no es abonado como corresponde.

No pasa lo mismo cuando tenemos una predicción de que un cliente es un mal pagador, cuando en realidad es bueno, ya que no tenemos más que un costo de oportunidad de no haber otorgado el crédito, pero una pérdida monetaria.

Ahora tendremos que verificar que las variables categóricas y numéricas estén bien clasificadas para evitar posibles errores de interpretación.
"""

numeric_columns = df.select_dtypes(include=['integer']).columns.tolist()
categorical_columns = df.select_dtypes(include=['object']).columns.tolist()

print("=== CLASIFICACIÓN DE VARIABLES ===")
print(f"Variables numéricas ({len(numeric_columns)}):")
for col in numeric_columns:
    print(f"  _{col}")

print(f"\nVariables categóricas ({len(categorical_columns)}):")
for col in categorical_columns:
    print(f"  _{col}")

"""Sin embargo, algunas variables no por tener números son necesariamente numéricas ya que pueden ser categóricas (como es el caso de class que sabemos que es la variable objetivo).

Para eso, podemos ver cuántos valores únicos tiene cada variable supuestamente numérica.
"""

print("=== VALORES ÚNICOS DE LAS COLUMNAS NUMÉRICAS ===")
for variables in numeric_columns:
  print(f"_{variables} tiene {df[variables].nunique()} valores únicos")

"""Ahora con esto y  con la documentación del dataset podemos comparar más fácil más fácilmente cuáles son variables categóricas y meterlas dentro del grupo correspondiente.

En este caso, la única variable mal catalogada es **class**.


"""

numeric_columns.remove('class')
numeric_columns

categorical_columns.append('class')
categorical_columns

print("=== CLASIFICACIÓN DE VARIABLES ===")
print(f"Variables numéricas ({len(numeric_columns)}):")
for col in numeric_columns:
    print(f"  _{col}")

print(f"\nVariables categóricas ({len(categorical_columns)}):")
for col in categorical_columns:
    print(f"  _{col}")

"""Ahora con los grupos bien separados, puedo separar de manera más sencilla el dataframe para hacer análisis de cada una de las posibles variables que tengo en él.

Primero podemos comenzar haciendo estadística descriptiva de las variables numéricas:
"""

df[numeric_columns].describe()

"""También podemos detectar posibles outliers (aunque si hay pocos no afectará tanto al modelo por utilizar un Random Forest que es robusto ante estos casos). Para detectarlos utilizaremos dos métodos:

*   Gráficamente
*   Analíticamente (IQR)


"""

df[numeric_columns].hist(bins=50, figsize=(20,15))
plt.show()

sns.pairplot(data=df[numeric_columns])

"""A simple vista no parece haber outliers muy grandes en las variables, a excepción de la varible duration_in_months. Sin embargo, es preferible conservarla debido a que es un factor importante en el riesgo crediticio (por más que no siga la distribución esperada). Esto se decide debido a que se espera que el Random Forest pueda ser capaz de soportar este problema por su robustez.

Ahora podemos analizar las variables categóricas.

Podemos comenzar viendo su distribución por medio de gráficos de barras.
"""

fig, axes = plt.subplots(5, 3, figsize=(15, 25))

for i, col in enumerate(categorical_columns):
    row = i // 3
    col_idx = i % 3

    value_counts = df[col].value_counts()
    value_counts.plot(kind='bar', ax=axes[row, col_idx],
                     color='paleturquoise', edgecolor='black')
    axes[row, col_idx].set_title(f'Distribución de {col}')
    axes[row, col_idx].set_xlabel(col)
    axes[row, col_idx].set_ylabel('Frecuencia')
    axes[row, col_idx].tick_params(axis='x', rotation=45)
    axes[row, col_idx].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

"""Hay 2 variables que hacen ruido pro su distribución:

*   other_debtors
*   foreign_worker

Esto debido a que solo 1 de sus valores tiene mucha predominancia por sobre el resto y puede generar overfitting. Por lo que conviene descartarlas ya que no aportan mucho al modelo y pueden alterarlo.


"""

df.drop(columns=['other_debtors','foreign_worker'])

"""Ahora es buena idea armar una matriz de correlación."""

corr_matrix = df[numeric_columns].corr()
plt.figure(figsize=(4,4))
plt.imshow(corr_matrix, cmap='seismic')

xt = plt.xticks(np.arange(6), df[numeric_columns].columns[:-1], rotation=45, ha='right', va='top')
yt = plt.yticks(np.arange(6), df[numeric_columns].columns[:-1], rotation=0, ha='right', va='center')

plt.colorbar(label='Pearson CC')

"""Podemos notar un problema de multicolinealidad entre las variables credit_amount y duration_in_months (R2 de 0.7). Sin embargo, no haremos modificaciones por 2 motivos:

*   No es muy elevado el coeficiente de Pearson
*   El modelo Random Forest es robusto ante estos casos

# 3) Entrenamiento de Modelo Simple (sin GridSearchCV)

Primero tenemos que usar un encoder para poder procesar el dataframe de las features. Usaremos getdummies para ello.
"""

X = df.drop(columns=['class'])
X = pd.get_dummies(X)
X

y = df['class']
y

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)

rf_simple = RandomForestClassifier(max_depth=5, random_state=42)
rf_simple.fit(X_train, y_train)

"""Para poder medir el modelo, utilizaremos la métrica Roc-Auc Score debido a que necesitamos penalizar más los falsos buenos pagadores, como ya fue explicado previamente."""

y_true = y_test
y_pred = rf_simple.predict(X_test)
roc_auc = roc_auc_score(y_true, y_pred)
print(f"Roc-Auc score: {roc_auc}")

"""Es bueno armar una matriz de confusión para detectar que no haya tantas predicciones de buenos pagadores que en realidad son malos."""

cm = confusion_matrix(y_true, y_pred)
cm_df = pd.DataFrame(cm,
                     index=['Real: 1', 'Real: 2'],
                     columns=['Predicho: 1', 'Predicho: 2'])

print("Matriz de Confusión:")
print(f'\n{cm_df}')

"""Acá vemos que hay muchos errores que no queremos. Tenemos 37 malos pagadores que fueron detectados como buenos. Es un mal resultado para nuestro objetivo.

# 4) Entrenamiento de Modelo Optimizado (con GridSearchCV)

Ahora armamos un modelo con los hiperparámetros optimizados para un mejor resultado.
"""

rf_omptimized = RandomForestClassifier(random_state=18)

parameters = {'n_estimators': [100,200],
              'max_depth': [5, 7, 10],
              'min_samples_split': [10, 20],
              'min_samples_leaf': [5, 10],
              'class_weight': [{1: 1, 2: 5}]}
rf_optimized = GridSearchCV(rf_omptimized, parameters, cv= 5, n_jobs= -1, scoring= 'roc_auc')
rf_optimized.fit(X_train, y_train)

pd.DataFrame(rf_optimized.cv_results_).sort_values(by='rank_test_score')

y_true = y_test
y_pred = rf_optimized.predict(X_test)
roc_auc = roc_auc_score(y_true, y_pred)
print(f"Roc-Auc score: {roc_auc}")

cm = confusion_matrix(y_true, y_pred)
cm_df = pd.DataFrame(cm,
                     index=['Real: 1', 'Real: 2'],
                     columns=['Predicho: 1', 'Predicho: 2'])

print("Matriz de Confusión:")
print(f'\n{cm_df}')

"""Acá la matriz de confusión da un mejor resultado. Solo tenemos 3 malos pagadores detectados como buenos. Sin embargo, tenemos 45 casos al revés (pero no es tan relevante). El objetivo principal se cumple.

# 5) Deployment

Exportamos el modelo como un .pkl para poder cargarlo a Streamlit
"""

# Guardar modelo
joblib.dump(rf_optimized.best_estimator_, 'modelo_credito.pkl')

# Descargar automáticamente
files.download('modelo_credito.pkl')